{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://imgur.com/1ZcRyrc.png\" style=\"float: left; margin: 20px; height: 55px\">\n",
    "\n",
    "# Supervised Learning Model Comparison\n",
    "\n",
    "---\n",
    "\n",
    "### Let us begin...\n",
    "\n",
    "Recall the `data science process`.\n",
    "   1. Define the problem.\n",
    "   2. Gather the data.\n",
    "   3. Explore the data.\n",
    "   4. Model the data.\n",
    "   5. Evaluate the model.\n",
    "   6. Answer the problem.\n",
    "\n",
    "In this lab, we're going to focus mostly on creating (and then comparing) many regression and classification models. Thus, we'll define the problem and gather the data for you.\n",
    "Most of the questions requiring a written response can be written in 2-3 sentences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Define the problem.\n",
    "\n",
    "You are a data scientist with a financial services company. Specifically, you want to leverage data in order to identify potential customers.\n",
    "\n",
    "If you are unfamiliar with \"401(k)s\" or \"IRAs,\" these are two types of retirement accounts. Very broadly speaking:\n",
    "- You can put money for retirement into both of these accounts.\n",
    "- The money in these accounts gets invested and hopefully has a lot more money in it when you retire.\n",
    "- These are a little different from regular bank accounts in that there are certain tax benefits to these accounts. Also, employers frequently match money that you put into a 401k.\n",
    "- If you want to learn more about them, check out [this site](https://www.nerdwallet.com/article/ira-vs-401k-retirement-accounts).\n",
    "\n",
    "We will tackle one regression problem and one classification problem today.\n",
    "- Regression: What features best predict one's income?\n",
    "- Classification: Predict whether or not one is eligible for a 401k.\n",
    "\n",
    "Check out the data dictionary [here](http://fmwww.bc.edu/ec-p/data/wooldridge2k/401KSUBS.DES).\n",
    "\n",
    "#### NOTE: When predicting `inc`, you should pretend as though you do not have access to the `e401k`, the `p401k` variable, and the `pira` variable. \n",
    "\n",
    "#### When predicting `e401k`, you may use the entire dataframe if you wish."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Gather the data.\n",
    "\n",
    "##### 1. Read in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# library imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression, Lasso, Ridge, ElasticNet, LinearRegression\n",
    "from sklearn.tree import DecisionTreeRegressor, DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor\n",
    "from sklearn.ensemble import BaggingRegressor, BaggingClassifier,\\\n",
    "RandomForestRegressor, RandomForestClassifier, AdaBoostRegressor, AdaBoostClassifier\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn import metrics\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('401ksubs.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>e401k</th>\n",
       "      <th>inc</th>\n",
       "      <th>marr</th>\n",
       "      <th>male</th>\n",
       "      <th>age</th>\n",
       "      <th>fsize</th>\n",
       "      <th>nettfa</th>\n",
       "      <th>p401k</th>\n",
       "      <th>pira</th>\n",
       "      <th>incsq</th>\n",
       "      <th>agesq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>13.170</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>1</td>\n",
       "      <td>4.575</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>173.4489</td>\n",
       "      <td>1600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>61.230</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>35</td>\n",
       "      <td>1</td>\n",
       "      <td>154.000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3749.1130</td>\n",
       "      <td>1225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>12.858</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>44</td>\n",
       "      <td>2</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>165.3282</td>\n",
       "      <td>1936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>98.880</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>44</td>\n",
       "      <td>2</td>\n",
       "      <td>21.800</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9777.2540</td>\n",
       "      <td>1936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>22.614</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>53</td>\n",
       "      <td>1</td>\n",
       "      <td>18.450</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>511.3930</td>\n",
       "      <td>2809</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   e401k     inc  marr  male  age  fsize   nettfa  p401k  pira      incsq  \\\n",
       "0      0  13.170     0     0   40      1    4.575      0     1   173.4489   \n",
       "1      1  61.230     0     1   35      1  154.000      1     0  3749.1130   \n",
       "2      0  12.858     1     0   44      2    0.000      0     0   165.3282   \n",
       "3      0  98.880     1     1   44      2   21.800      0     0  9777.2540   \n",
       "4      0  22.614     0     0   53      1   18.450      0     0   511.3930   \n",
       "\n",
       "   agesq  \n",
       "0   1600  \n",
       "1   1225  \n",
       "2   1936  \n",
       "3   1936  \n",
       "4   2809  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "e401k       int64\n",
       "inc       float64\n",
       "marr        int64\n",
       "male        int64\n",
       "age         int64\n",
       "fsize       int64\n",
       "nettfa    float64\n",
       "p401k       int64\n",
       "pira        int64\n",
       "incsq     float64\n",
       "agesq       int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9275, 11)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>e401k</th>\n",
       "      <th>inc</th>\n",
       "      <th>marr</th>\n",
       "      <th>male</th>\n",
       "      <th>age</th>\n",
       "      <th>fsize</th>\n",
       "      <th>nettfa</th>\n",
       "      <th>p401k</th>\n",
       "      <th>pira</th>\n",
       "      <th>incsq</th>\n",
       "      <th>agesq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>9275.000000</td>\n",
       "      <td>9275.000000</td>\n",
       "      <td>9275.000000</td>\n",
       "      <td>9275.000000</td>\n",
       "      <td>9275.000000</td>\n",
       "      <td>9275.000000</td>\n",
       "      <td>9275.000000</td>\n",
       "      <td>9275.000000</td>\n",
       "      <td>9275.000000</td>\n",
       "      <td>9275.000000</td>\n",
       "      <td>9275.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.392129</td>\n",
       "      <td>39.254641</td>\n",
       "      <td>0.628571</td>\n",
       "      <td>0.204420</td>\n",
       "      <td>41.080216</td>\n",
       "      <td>2.885067</td>\n",
       "      <td>19.071675</td>\n",
       "      <td>0.276226</td>\n",
       "      <td>0.254340</td>\n",
       "      <td>2121.192483</td>\n",
       "      <td>1793.652722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.488252</td>\n",
       "      <td>24.090002</td>\n",
       "      <td>0.483213</td>\n",
       "      <td>0.403299</td>\n",
       "      <td>10.299517</td>\n",
       "      <td>1.525835</td>\n",
       "      <td>63.963838</td>\n",
       "      <td>0.447154</td>\n",
       "      <td>0.435513</td>\n",
       "      <td>3001.469424</td>\n",
       "      <td>895.648841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>10.008000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-502.302000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>100.160100</td>\n",
       "      <td>625.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>21.660000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>33.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>-0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>469.155600</td>\n",
       "      <td>1089.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>33.288000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1108.091000</td>\n",
       "      <td>1600.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>50.160000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>48.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>18.449500</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2516.025500</td>\n",
       "      <td>2304.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>199.041000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>64.000000</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>1536.798000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>39617.320000</td>\n",
       "      <td>4096.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             e401k          inc         marr         male          age  \\\n",
       "count  9275.000000  9275.000000  9275.000000  9275.000000  9275.000000   \n",
       "mean      0.392129    39.254641     0.628571     0.204420    41.080216   \n",
       "std       0.488252    24.090002     0.483213     0.403299    10.299517   \n",
       "min       0.000000    10.008000     0.000000     0.000000    25.000000   \n",
       "25%       0.000000    21.660000     0.000000     0.000000    33.000000   \n",
       "50%       0.000000    33.288000     1.000000     0.000000    40.000000   \n",
       "75%       1.000000    50.160000     1.000000     0.000000    48.000000   \n",
       "max       1.000000   199.041000     1.000000     1.000000    64.000000   \n",
       "\n",
       "             fsize       nettfa        p401k         pira         incsq  \\\n",
       "count  9275.000000  9275.000000  9275.000000  9275.000000   9275.000000   \n",
       "mean      2.885067    19.071675     0.276226     0.254340   2121.192483   \n",
       "std       1.525835    63.963838     0.447154     0.435513   3001.469424   \n",
       "min       1.000000  -502.302000     0.000000     0.000000    100.160100   \n",
       "25%       2.000000    -0.500000     0.000000     0.000000    469.155600   \n",
       "50%       3.000000     2.000000     0.000000     0.000000   1108.091000   \n",
       "75%       4.000000    18.449500     1.000000     1.000000   2516.025500   \n",
       "max      13.000000  1536.798000     1.000000     1.000000  39617.320000   \n",
       "\n",
       "             agesq  \n",
       "count  9275.000000  \n",
       "mean   1793.652722  \n",
       "std     895.648841  \n",
       "min     625.000000  \n",
       "25%    1089.000000  \n",
       "50%    1600.000000  \n",
       "75%    2304.000000  \n",
       "max    4096.000000  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "e401k     0\n",
       "inc       0\n",
       "marr      0\n",
       "male      0\n",
       "age       0\n",
       "fsize     0\n",
       "nettfa    0\n",
       "p401k     0\n",
       "pira      0\n",
       "incsq     0\n",
       "agesq     0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2. What are 2-3 other variables that, if available, would be helpful to have?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Years of Experience, Net Debt, Employment Status, Type of Work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3. Suppose a peer recommended putting `race` into your model in order to better predict who to target when advertising IRAs and 401(k)s. Why would this be an unethical decision?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using race in the model might lead to discriminatory practices. \n",
    "# Basing predictions on race could reinforce biases and limit access to financial resources and information for certain racial groups."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Explore the data.\n",
    "\n",
    "##### 4. When attempting to predict income, which feature(s) would we reasonably not use? Why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inches not used in income prediction model because inches or how tall they are doesn't meanning with income"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 5. What two variables have already been created for us through feature engineering? Come up with a hypothesis as to why subject-matter experts may have done this.\n",
    "> This need not be a \"statistical hypothesis.\" Just brainstorm why SMEs (Subject Matter Experts) might have done this!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# agesq, incsq came from feature engineering with the reason that, the age and inches "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop(columns = ['agesq', 'incsq'], inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 6. Looking at the data dictionary, one variable description appears to be an error. What is this error, and what do you think the correct value would be?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inc is described as inches^2, but it may actually represent income."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Model the data. (Part 1: Regression Problem)\n",
    "\n",
    "Recall:\n",
    "- Problem: What features best predict one's income?\n",
    "- When predicting `inc`, you should pretend as though you do not have access to the `e401k`, the `p401k` variable, and the `pira` variable.\n",
    "\n",
    "##### 7. List all modeling tactics we've learned that could be used to solve a regression problem (as of Wednesday afternoon of Week 6). For each tactic, identify whether it is or is not appropriate for solving this specific regression problem and explain why or why not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Multiple Linear Regression: A good choice for simplicity and interpretability. (**appropriate**)\n",
    "2. Decision Trees: Handles non-linear relationships and interactions well but can overfit. (**appropriate**)\n",
    "3. Random Forests: Reduces the overfitting tendency of decision trees by averaging multiple trees, though less interpretable. (**appropriate**)\n",
    "4. k-Nearest Neighbors (kNN): Sensitive to the scale of features and works better with low-dimensional data. (**appropriate**)\n",
    "5. Gradient Boosting Machines: Offers strong predictive power and handles complex relationships, though less interpretable. (**appropriate**)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 8. Regardless of your answer to number 7, fit at least one of each of the following models to attempt to solve the regression problem above:\n",
    "    - a multiple linear regression model\n",
    "    - a k-nearest neighbors model\n",
    "    - a decision tree\n",
    "    - a set of bagged decision trees\n",
    "    - a random forest\n",
    "    - an Adaboost model\n",
    "    \n",
    "> As always, be sure to do a train/test split! In order to compare modeling techniques, you should use the same train-test split on each. I recommend setting a random seed here.\n",
    "\n",
    "> You may find it helpful to set up a pipeline to try each modeling technique, but you are not required to do so!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.drop(columns = ['e401k', 'p401k' ,'pira','inc'])\n",
    "y = data['inc']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 9. What is bootstrapping?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Create multiple small datasets from an original dataset by sampling with replacement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 10. What is the difference between a decision tree and a set of bagged decision trees? Be specific and precise!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision tree ---> A single model that splits the data based on feature values to make predictions. \n",
    "# Set of bagged decision trees ---> An ensemble of decision trees created by training multiple trees of the original dataset. The final prediction is majority voting (for classification) across all trees."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 11. What is the difference between a set of bagged decision trees and a random forest? Be specific and precise!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set of bagged decision trees --->  Collection of trees on bootstrapped data, reducing overfitting by averaging/voting.\n",
    "# Random Forest ---> Bagged decision trees with additional feature randomness, reducing overfitting even further for more reliable predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 12. Why might a random forest be superior to a set of bagged decision trees?\n",
    "> Hint: Consider the bias-variance tradeoff."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduce correlation among trees: Each tree considers only a random subset of features, making it less likely to produce similar splits.\n",
    "# Less similar splits across trees reduce variance and improve accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Evaluate the model. (Part 1: Regression Problem)\n",
    "\n",
    "##### 13. Using RMSE, evaluate each of the models you fit on both the training and testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "Regressor = [\n",
    "    ('Linear Regression', LinearRegression()),\n",
    "    ('k-Nearest Neighbors', KNeighborsRegressor()),\n",
    "    ('Decision Tree', DecisionTreeRegressor()),\n",
    "    ('Bagged Decision Trees', BaggingRegressor()),\n",
    "    ('Random Forest', RandomForestRegressor()),\n",
    "    ('AdaBoost',  AdaBoostRegressor())\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Regression model\n",
      "Training RMSE: 20.4019\n",
      "Testing RMSE: 21.4684\n",
      "\n",
      "k-Nearest Neighbors model\n",
      "Training RMSE: 16.5042\n",
      "Testing RMSE: 20.2338\n",
      "\n",
      "Decision Tree model\n",
      "Training RMSE: 2.2981\n",
      "Testing RMSE: 27.3855\n",
      "\n",
      "Bagged Decision Trees model\n",
      "Training RMSE: 8.7880\n",
      "Testing RMSE: 20.9266\n",
      "\n",
      "Random Forest model\n",
      "Training RMSE: 7.6859\n",
      "Testing RMSE: 20.4184\n",
      "\n",
      "AdaBoost model\n",
      "Training RMSE: 22.4442\n",
      "Testing RMSE: 23.4737\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def compare_regression(models):\n",
    "    \n",
    "    for name,model in models:        \n",
    "        pipeline = Pipeline([\n",
    "            ('scaler', StandardScaler()),        \n",
    "            (f'{model}', model)           \n",
    "        ])\n",
    "\n",
    "        pipeline.fit(X_train, y_train)\n",
    "\n",
    "        y_train_pred = pipeline.predict(X_train)\n",
    "        y_test_pred = pipeline.predict(X_test)\n",
    "\n",
    "        rmse_train = metrics.root_mean_squared_error(y_train, y_train_pred)\n",
    "        rmse_test = metrics.root_mean_squared_error(y_test, y_test_pred)\n",
    "\n",
    "        print(f\"{name} model\")\n",
    "        print(f\"Training RMSE: {rmse_train:.4f}\")\n",
    "        print(f\"Testing RMSE: {rmse_test:.4f}\")\n",
    "        print()\n",
    "        \n",
    "compare_regression(Regressor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 14. Based on training RMSE and testing RMSE, is there evidence of overfitting in any of your models? Which ones?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overfitting is detected in the Decision Tree, Bagged Decision Trees, Random Forest  (strong overfitting with a training RMSE)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 15. Based on everything we've covered so far, if you had to pick just one model as your final model to use to answer the problem in front of you, which one model would you pick? Defend your choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The Linear Regression model \n",
    "# The reason : this model performs well on both training and testing data, with minimal difference between the RMSE values \n",
    "# (Training RMSE: 20.4019, Testing RMSE: 21.4684). \n",
    "# This indicates that it does not overfit the training data, making it a reliable model for generalization to new, unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 16. Suppose you wanted to improve the performance of your final model. Brainstorm 2-3 things that, if you had more time, you would attempt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 5 candidates, totalling 25 fits\n",
      "Best Estimator: Ridge(alpha=0.01)\n",
      "Training RMSE: 20.4019\n",
      "Testing RMSE: 21.4684\n",
      "\n",
      "Fitting 5 folds for each of 5 candidates, totalling 25 fits\n",
      "Best Estimator: Lasso(alpha=0.01)\n",
      "Training RMSE: 20.4019\n",
      "Testing RMSE: 21.4690\n",
      "\n",
      "Fitting 5 folds for each of 5 candidates, totalling 25 fits\n",
      "Best Estimator: ElasticNet(alpha=0.01)\n",
      "Training RMSE: 20.4039\n",
      "Testing RMSE: 21.4716\n",
      "\n",
      "LinearRegression Best Estimator\n",
      "Training RMSE: 20.4019\n",
      "Testing RMSE: 21.4684\n"
     ]
    }
   ],
   "source": [
    "regulars = [Ridge(), Lasso(), ElasticNet()]\n",
    "params = {'alpha': [0.01 ,0.1, 1, 10, 100]}\n",
    "\n",
    "for regular in regulars:\n",
    "    gridsearch = GridSearchCV(regular, params, cv=5, verbose=1)\n",
    "    gridsearch.fit(X_train, y_train)\n",
    "\n",
    "    y_train_pred = gridsearch.predict(X_train)\n",
    "    y_test_pred = gridsearch.predict(X_test)\n",
    "\n",
    "    rmse_train = metrics.root_mean_squared_error(y_train, y_train_pred)\n",
    "    rmse_test = metrics.root_mean_squared_error(y_test, y_test_pred)\n",
    "\n",
    "    print(\"Best Estimator:\", gridsearch.best_estimator_)\n",
    "    print(f\"Training RMSE: {rmse_train:.4f}\")\n",
    "    print(f\"Testing RMSE: {rmse_test:.4f}\")\n",
    "    print()\n",
    "    \n",
    "lr = LinearRegression()\n",
    "lr.fit(X_train, y_train)\n",
    "linear_score = lr.score(X_train, y_train)\n",
    "y_train_pred = lr.predict(X_train)\n",
    "y_test_pred = lr.predict(X_test)\n",
    "\n",
    "rmse_train = metrics.root_mean_squared_error(y_train, y_train_pred)\n",
    "\n",
    "print(\"LinearRegression Best Estimator\")\n",
    "rmse_test = metrics.root_mean_squared_error(y_test, y_test_pred)\n",
    "print(f\"Training RMSE: {rmse_train:.4f}\")\n",
    "print(f\"Testing RMSE: {rmse_test:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training RMSE: 20.4019\n",
      "Testing RMSE: 21.4684\n"
     ]
    }
   ],
   "source": [
    "# Normal Linear Regression has the best score.\n",
    "lr = LinearRegression()\n",
    "lr.fit(X_train,y_train)\n",
    "y_train_pred = lr.predict(X_train)\n",
    "y_test_pred = lr.predict(X_test)\n",
    "\n",
    "rmse_train = metrics.root_mean_squared_error(y_train, y_train_pred)\n",
    "rmse_test = metrics.root_mean_squared_error(y_test, y_test_pred)\n",
    "\n",
    "print(f\"Training RMSE: {rmse_train:.4f}\")\n",
    "print(f\"Testing RMSE: {rmse_test:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feature</th>\n",
       "      <th>Coefficient</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>marr</td>\n",
       "      <td>20.290498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>male</td>\n",
       "      <td>3.075914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>age</td>\n",
       "      <td>0.047262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>fsize</td>\n",
       "      <td>-1.508391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>nettfa</td>\n",
       "      <td>0.131868</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Feature  Coefficient\n",
       "0    marr    20.290498\n",
       "1    male     3.075914\n",
       "2     age     0.047262\n",
       "3   fsize    -1.508391\n",
       "4  nettfa     0.131868"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_coef = pd.DataFrame({'Feature': X.columns,'Coefficient': lr.coef_})\n",
    "df_coef"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Model the data. (Part 2: Classification Problem)\n",
    "\n",
    "Recall:\n",
    "- Problem: Predict whether or not one is eligible for a 401k.\n",
    "- When predicting `e401k`, you may use the entire dataframe if you wish.\n",
    "\n",
    "##### 17. While you're allowed to use every variable in your dataframe, mention at least one disadvantage of using `p401k` in your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The reason is that before participating in a 401k, they must be eligible for it. \n",
    "# If using p401k in the model, it can lead to incorrect conclusions, \n",
    "# such as assuming they are participating in a 401k when they are not eligible for it. This is not true."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 18. List all modeling tactics we've learned that could be used to solve a classification problem (as of Wednesday afternoon of Week 6). For each tactic, identify whether it is or is not appropriate for solving this specific classification problem and explain why or why not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Logistic Regression: This model is for binary classification due to its simplicity and interpretability. (**appropriate**)\n",
    "2. Decision Trees: This model is especially for classification but can overfit. (**appropriate**)\n",
    "3. Random Forests: Reduces the overfitting tendency of decision trees by averaging multiple trees, though it is less interpretable. (**appropriate**)\n",
    "4. k-Nearest Neighbors (kNN): Sensitive to the scale of features and works better with low-dimensional data. (**appropriate**)\n",
    "5. AdaBoost model: This model enhances weak learners like decision trees. (**appropriate**)\n",
    "6. Gradient Boosting: This model builds an ensemble of weak learners sequentially, with strong predictive performance but is less interpretable. (**appropriate**)\n",
    "7. XGBoost model: This is an optimized version of Gradient Boosting that is widely used for classification tasks. (**appropriate**)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 19. Regardless of your answer to number 18, fit at least one of each of the following models to attempt to solve the classification problem above:\n",
    "    - a logistic regression model\n",
    "    - a k-nearest neighbors model\n",
    "    - a decision tree\n",
    "    - a set of bagged decision trees\n",
    "    - a random forest\n",
    "    - an Adaboost model\n",
    "    \n",
    "> As always, be sure to do a train/test split! In order to compare modeling techniques, you should use the same train-test split on each. I recommend using a random seed here.\n",
    "\n",
    "> You may find it helpful to set up a pipeline to try each modeling technique, but you are not required to do so!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "e401k\n",
       "0    0.607871\n",
       "1    0.392129\n",
       "Name: proportion, dtype: float64"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['e401k'].value_counts(normalize = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.drop(columns = ['p401k','e401k'])\n",
    "y = data['e401k']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X,y, random_state=42, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((6956, 7), (2319, 7), (6956,), (2319,))"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "e401k\n",
       "0    0.607821\n",
       "1    0.392179\n",
       "Name: proportion, dtype: float64"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.value_counts(normalize = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "e401k\n",
       "0    0.608021\n",
       "1    0.391979\n",
       "Name: proportion, dtype: float64"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.value_counts(normalize = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "Classifier = [\n",
    "    ('Logistic Regression', LogisticRegression(solver='liblinear', penalty='l2', C=1.0, max_iter=1000)),\n",
    "    ('k-Nearest Neighbors', KNeighborsClassifier()),\n",
    "    ('Decision Tree', DecisionTreeClassifier()),\n",
    "    ('Bagged Decision Trees', BaggingClassifier()),\n",
    "    ('Random Forest', RandomForestClassifier()),\n",
    "    ('AdaBoost',  AdaBoostClassifier(algorithm='SAMME'))\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression model\n",
      "Cross Validation score 0.6489\n",
      "Training score: 0.6486\n",
      "Testing score: 0.6542\n",
      "\n",
      "k-Nearest Neighbors model\n",
      "Cross Validation score 0.6333\n",
      "Training score: 0.7542\n",
      "Testing score: 0.6266\n",
      "\n",
      "Decision Tree model\n",
      "Cross Validation score 0.5966\n",
      "Training score: 1.0000\n",
      "Testing score: 0.5826\n",
      "\n",
      "Bagged Decision Trees model\n",
      "Cross Validation score 0.6429\n",
      "Training score: 0.9770\n",
      "Testing score: 0.6481\n",
      "\n",
      "Random Forest model\n",
      "Cross Validation score 0.6686\n",
      "Training score: 1.0000\n",
      "Testing score: 0.6636\n",
      "\n",
      "AdaBoost model\n",
      "Cross Validation score 0.6801\n",
      "Training score: 0.6875\n",
      "Testing score: 0.6900\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def compare_classification(models):\n",
    "    \n",
    "    f1_list = []\n",
    "    \n",
    "    for name,model in models:        \n",
    "        pipeline = Pipeline([\n",
    "            ('scaler', StandardScaler()),        \n",
    "            ('model', model)           \n",
    "        ])\n",
    "\n",
    "        pipeline.fit(X_train, y_train)\n",
    "\n",
    "        cv_scores = cross_val_score(model, X_train, y_train, cv=5)\n",
    "        train_score = pipeline.score(X_train, y_train)\n",
    "        test_score = pipeline.score(X_test, y_test)\n",
    "\n",
    "        # Predict on training and testing data\n",
    "        train_pred = pipeline.predict(X_train)\n",
    "        test_pred = pipeline.predict(X_test)\n",
    "\n",
    "        train_f1 = metrics.f1_score(y_train, train_pred)\n",
    "        test_f1 = metrics.f1_score(y_test, test_pred)\n",
    "\n",
    "        print(f\"{name} model\")\n",
    "        print(f\"Cross Validation score {cv_scores.mean():.4f}\")\n",
    "        print(f\"Training score: {train_score:.4f}\")\n",
    "        print(f\"Testing score: {test_score:.4f}\")\n",
    "        \n",
    "        print()\n",
    "\n",
    "        # Append results to the list\n",
    "        f1_list.append({\n",
    "            'model': name,\n",
    "            'train f1': train_f1,\n",
    "            'test f1': test_f1\n",
    "        })\n",
    "        \n",
    "    # Convert the list to a DataFrame\n",
    "    f1_df = pd.DataFrame(f1_list)\n",
    "\n",
    "    return f1_df\n",
    "        \n",
    "\n",
    "f1_score = compare_classification(Classifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Evaluate the model. (Part 2: Classfication Problem)\n",
    "\n",
    "##### 20. Suppose our \"positive\" class is that someone is eligible for a 401(k). What are our false positives? What are our false negatives?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# False positives: Individuals who are not eligible for a 401k but are predicted to be eligible.\n",
    "# False negatives: Individuals who are eligible for a 401k but are predicted not to be eligible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 21. In this specific case, would we rather minimize false positives or minimize false negatives? Defend your choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Minimizing false negatives to ensure that all eligible individuals are given the opportunity to participate in the 401k. \n",
    "# It's better than tell someone they are eligible (false positive)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 22. Suppose we wanted to optimize for the answer you provided in problem 21. Which metric would we optimize in this case?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimize sensitivity (also known as recall)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 23. Suppose that instead of optimizing for the metric in problem 21, we wanted to balance our false positives and false negatives using `f1-score`. Why might [f1-score](https://en.wikipedia.org/wiki/F1_score) be an appropriate metric to use here?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f1-score gives a better sense of how well the model performs in terms of both \n",
    "# identifying true positives (sensitivity) and avoiding false positives (precision)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 24. Using f1-score, evaluate each of the models you fit on both the training and testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>train f1</th>\n",
       "      <th>test f1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.374616</td>\n",
       "      <td>0.391502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>k-Nearest Neighbors</td>\n",
       "      <td>0.658819</td>\n",
       "      <td>0.475787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.476757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Bagged Decision Trees</td>\n",
       "      <td>0.969947</td>\n",
       "      <td>0.487437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.527273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>AdaBoost</td>\n",
       "      <td>0.584321</td>\n",
       "      <td>0.585113</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   model  train f1   test f1\n",
       "0    Logistic Regression  0.374616  0.391502\n",
       "1    k-Nearest Neighbors  0.658819  0.475787\n",
       "2          Decision Tree  1.000000  0.476757\n",
       "3  Bagged Decision Trees  0.969947  0.487437\n",
       "4          Random Forest  1.000000  0.527273\n",
       "5               AdaBoost  0.584321  0.585113"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 25. Based on training f1-score and testing f1-score, is there evidence of overfitting in any of your models? Which ones?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overfitting is most likely present in Decision Tree, Random Forest, \n",
    "# and possibly Bagged Decision Trees, based on the significant disparity between their training and testing f1-scores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 26. Based on everything we've covered so far, if you had to pick just one model as your final model to use to answer the problem in front of you, which one model would you pick? Defend your choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Although Random Forest and Decision Trees may have higher training performance, they suffer from overfitting. \n",
    "# On the other hand, AdaBoost strikes a good balance between training and testing performance, \n",
    "# making it the most suitable model for reliable predictions in this case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 27. Suppose you wanted to improve the performance of your final model. Brainstorm 2-3 things that, if you had more time, you would attempt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n",
      "Best Score: 0.6838702413710471\n",
      "Best Params: {'estimator__max_depth': 2, 'learning_rate': 1.0, 'n_estimators': 30}\n",
      "Training f1-score: 0.5709\n",
      "Testing f1-score: 0.5764\n",
      "How long did all take to run: 62 s\n"
     ]
    }
   ],
   "source": [
    "t0 = time.time()\n",
    "\n",
    "ada = AdaBoostClassifier(estimator=DecisionTreeClassifier(),algorithm='SAMME')\n",
    "params = {\n",
    "    'n_estimators' : [30,50,100],\n",
    "    'estimator__max_depth' : [1,2],\n",
    "    'learning_rate': [0.7,0.9,1.0],\n",
    "}\n",
    "\n",
    "gridsearch = GridSearchCV(ada, params, cv =5 , verbose=1)\n",
    "\n",
    "gridsearch.fit(X_train,y_train)\n",
    "\n",
    "# Print best results\n",
    "print(\"Best Score:\", gridsearch.best_score_)\n",
    "print(\"Best Params:\", gridsearch.best_params_)\n",
    "\n",
    "# Predict using the best model\n",
    "train_pred = gridsearch.best_estimator_.predict(X_train)\n",
    "test_pred = gridsearch.best_estimator_.predict(X_test)\n",
    "\n",
    "# Compute F1 scores\n",
    "train_f1 = metrics.f1_score(y_train, train_pred)\n",
    "test_f1 = metrics.f1_score(y_test, test_pred)\n",
    "\n",
    "# Output F1 scores\n",
    "print(f\"Training f1-score: {train_f1:.4f}\")\n",
    "print(f\"Testing f1-score: {test_f1:.4f}\")\n",
    "\n",
    "# Print total runtime\n",
    "print(f\"How long did all take to run: {(time.time() - t0):.0f} s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy score: 0.6913\n",
      "Testing accuracy score: 0.6964\n",
      "\n",
      "  feature  importance\n",
      "5  nettfa    0.534129\n",
      "0     inc    0.327972\n",
      "3     age    0.066109\n",
      "1    marr    0.029835\n",
      "6    pira    0.023770\n",
      "4   fsize    0.018186\n",
      "2    male    0.000000\n"
     ]
    }
   ],
   "source": [
    "# Normal Adaboost has the best score.\n",
    "ada = AdaBoostClassifier( estimator= DecisionTreeClassifier(max_depth=2),\n",
    "                         algorithm='SAMME', learning_rate= 1.0, n_estimators= 30)\n",
    "\n",
    "ada.fit(X_train,y_train)\n",
    "train_pred = ada.predict(X_train)\n",
    "test_pred = ada.predict(X_test)\n",
    "\n",
    "train_acc = metrics.accuracy_score(y_train, train_pred)\n",
    "test_acc = metrics.accuracy_score(y_test, test_pred)\n",
    "\n",
    "print(f\"Training accuracy score: {train_acc:.4f}\")\n",
    "print(f\"Testing accuracy score: {test_acc:.4f}\\n\")\n",
    "\n",
    "feature_importances_df = pd.DataFrame({'feature': X.columns, 'importance': ada.feature_importances_})\\\n",
    ".sort_values('importance', ascending=  False)\n",
    "\n",
    "print(feature_importances_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Answer the problem.\n",
    "\n",
    "##### BONUS: Briefly summarize your answers to the regression and classification problems. Be sure to include any limitations or hesitations in your answer.\n",
    "\n",
    "- Regression: What features best predict one's income?\n",
    "- Classification: Predict whether or not one is eligible for a 401k."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regression\n",
    "# Marital status has the most significant impact on predicting income. \n",
    "# If an individual is married, their income may increase by 20. Additionally, \n",
    "# for every additional 1 million in net total financial assets, income rises by 131.868.\n",
    "\n",
    "# Classification\n",
    "# For predicting eligibility for a 401(k) plan, net total financial assets (nettfa) is the most influential feature. \n",
    "# Higher net total financial assets are associated with a greater probability of eligibility for a 401(k) plan.\n",
    "\n",
    "#Limitations\n",
    "# The dataset has limited features for predicting 401(k) plan eligibility, including only income, family size, \n",
    "# net total financial assets, age, marital status, and sex. Adding features such as job contract type (e.g., permanent, temporary, freelance) \n",
    "# and years of work experience could improve model accuracy and insights."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
